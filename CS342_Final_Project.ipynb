{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c86b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these two blocks to load important libraries and set things up\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2dba9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # store list of file paths\n",
    "        self.file_paths = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith(\".txt\")]\n",
    "        def note_to_key(note):\n",
    "            notes = {\n",
    "                \"C\": 1,\n",
    "                \"B#\": 13,\n",
    "                \"C#\": 2,\n",
    "                \"B##\": 2,\n",
    "                \"Db\": 2,\n",
    "                \"D\": 3,\n",
    "                \"C##\": 3,\n",
    "                \"D#\": 4,\n",
    "                \"Eb\": 4,\n",
    "                \"E\": 5,\n",
    "                \"D##\": 5,\n",
    "                \"Fb\": 5,\n",
    "                \"F\": 6,\n",
    "                \"E#\": 6,\n",
    "                \"F#\": 7,\n",
    "                \"E##\": 7,\n",
    "                \"Gb\": 7,\n",
    "                \"G\": 8,\n",
    "                \"F##\": 8,\n",
    "                \"G#\": 9,\n",
    "                \"Ab\": 9,\n",
    "                \"A\": 10,\n",
    "                \"G##\": 10,\n",
    "                \"A#\": 11,\n",
    "                \"Bb\": 11,\n",
    "                \"B\": 12,\n",
    "                \"A##\": 12,\n",
    "                \"Cb\": -1\n",
    "            }\n",
    "            # B#5 = C6. C6 is 63 so octave = 5, note_name = B# = 13. \n",
    "            octave = int(note[-1])\n",
    "            note_name = note[:-1]\n",
    "            return int(((octave - 1) * 12) + notes[note_name] + 3)\n",
    "        \n",
    "        def make_label(lab):\n",
    "            if len(lab) <= 2:\n",
    "                return int(lab) + 5 if int(lab) < 0 else + 4# either left hand or right hand -1 or 1\n",
    "            else: # else would be substitution 3_1 or -3_-1 ? or is it -3_1\n",
    "                halves = lab.strip().split('_')\n",
    "                return int(halves[0]) + 6 if int(halves[0]) < 0 else int(halves[0]) + 4\n",
    "                \n",
    "        # store input and output sequences for each file separately\n",
    "        self.file_data = {}\n",
    "        for file_path in self.file_paths:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = [line.strip().split('\\t') for line in f]\n",
    "                data = data[1:] # trim header\n",
    "                input_seqs = [torch.tensor([note_to_key(x) for x in row[3].split()]) for row in data]\n",
    "                output_seqs = [torch.tensor([make_label(x) for x in row[7].split()]) for row in data]\n",
    "                self.file_data[file_path] = {'input': input_seqs, 'output': output_seqs}\n",
    "    \n",
    "        # compute the length of each file in the dataset\n",
    "        self.file_lengths = {}\n",
    "        for file_path in self.file_paths:\n",
    "            file_len = len(self.file_data[file_path]['input'])\n",
    "            self.file_lengths[file_path] = file_len - 9\n",
    "        \n",
    "        # compute the total length of the dataset\n",
    "        self.total_length = sum(self.file_lengths.values())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # find the file that contains the item at the given index\n",
    "        for file_path, file_len in self.file_lengths.items():\n",
    "            if idx < file_len:\n",
    "                break\n",
    "            idx -= file_len\n",
    "        \n",
    "        # get input and output sequences from the file starting from the given index\n",
    "        file_data = self.file_data[file_path]\n",
    "        input_seq = torch.stack(file_data['input'][idx:idx+10])\n",
    "        output_seq = torch.stack(file_data['output'][idx:idx+10])\n",
    "        return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c7a842",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../FingeringFiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_sampler \u001b[39m=\u001b[39m SubsetRandomSampler(train_ix)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m val_sampler \u001b[39m=\u001b[39m SubsetRandomSampler(val_ix)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_set \u001b[39m=\u001b[39m Seq2SeqDataset(\u001b[39m'\u001b[39;49m\u001b[39m../FingeringFiles\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_set \u001b[39m=\u001b[39m Seq2SeqDataset(\u001b[39m'\u001b[39m\u001b[39m../FingeringFiles\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n",
      "\u001b[1;32m/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb Cell 3\u001b[0m in \u001b[0;36mSeq2SeqDataset.__init__\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data_dir):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# store list of file paths\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, file) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(data_dir) \u001b[39mif\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mnote_to_key\u001b[39m(note):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         notes \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mB#\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m13\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCb\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macyhuang/finger-thinger/CS342_Final_Project.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         }\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../FingeringFiles'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "ntotal = 60000\n",
    "ntrain = int(0.9*ntotal)\n",
    "nval = ntotal - ntrain\n",
    "\n",
    "val_ix = np.random.choice(range(ntotal), size=nval, replace=False)\n",
    "train_ix = list(set(range(ntotal)) - set(val_ix))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_ix)\n",
    "val_sampler = SubsetRandomSampler(val_ix)\n",
    "\n",
    "train_set = Seq2SeqDataset('./FingeringFiles')\n",
    "test_set = Seq2SeqDataset('./FingeringFiles')\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(train_set, batch_size, sampler=val_sampler)\n",
    "test_loader = DataLoader(test_set, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b04cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hc_0):\n",
    "        # hc_0 needs to be a tuple of hidden and cell states, (h_0, c_0)\n",
    "        lstm_outputs, hc_n = self.lstm(x.to(torch.float32), (hc_0[0].to(torch.float32), hc_0[1].to(torch.float32)))\n",
    "        outputs = self.output_layer(lstm_outputs.to(torch.float32))\n",
    "        \n",
    "        return outputs, hc_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436004a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 32\n",
    "output_size = 10\n",
    "LSTModel = MyLSTM(input_size, hidden_size, output_size) # 88 keys on a piano, ten fingers\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(LSTModel.parameters(), lr=0.00001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accca009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_network(model, train_loader, val_loader, criterion, optimizer, nepoch):\n",
    "    try:\n",
    "        for epoch in range(nepoch):\n",
    "            print('EPOCH %d'%epoch)\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            hc_0 = (torch.zeros((1,10,hidden_size)), torch.zeros((1,10,hidden_size)))\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                # print(hc_0[0].detach().dtype)\n",
    "                outputs, hc_0 = model(inputs, (hc_0[0].detach(), hc_0[1].detach()))\n",
    "                # print(labels)\n",
    "                labels = F.one_hot(labels, 10).squeeze()\n",
    "                # print('slkjfklds', labels.shape, outputs.shape)\n",
    "                # print(labels)\n",
    "                \n",
    "                # print(outputs[-1])\n",
    "                loss = criterion(outputs[-1].to(torch.float32), labels.to(torch.float32))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                count = 0\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs, hc_0 = model(inputs, (hc_0[0].detach(), hc_0[1].detach()))\n",
    "                    labels = F.one_hot(labels, 10).squeeze()\n",
    "                    loss = criterion(outputs[-1], labels)\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "                print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
    "            print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting from training early')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 1\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 2\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 3\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 4\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 5\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 6\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 7\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 8\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 9\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 10\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 11\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 12\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 13\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 14\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 15\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 16\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 17\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 18\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 19\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 20\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 21\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 22\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05069\n",
      "\n",
      "EPOCH 23\n",
      " Train loss: 0.05038\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 24\n",
      " Train loss: 0.05038\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 25\n",
      " Train loss: 0.05038\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 26\n",
      " Train loss: 0.05037\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 27\n",
      " Train loss: 0.05037\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 28\n",
      " Train loss: 0.05037\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 29\n",
      " Train loss: 0.05037\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 30\n",
      " Train loss: 0.05036\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 31\n",
      " Train loss: 0.05037\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 32\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 33\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 34\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 35\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 36\n",
      " Train loss: 0.05036\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 37\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 38\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 39\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05069\n",
      "\n",
      "EPOCH 40\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 41\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 42\n",
      " Train loss: 0.05036\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 43\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 44\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 45\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 46\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 47\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 48\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 49\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 50\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05064\n",
      "\n",
      "EPOCH 51\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 52\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 53\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 54\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 55\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 56\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05063\n",
      "\n",
      "EPOCH 57\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 58\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 59\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 60\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 61\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 62\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 63\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 64\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05069\n",
      "\n",
      "EPOCH 65\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 66\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 67\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05064\n",
      "\n",
      "EPOCH 68\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05064\n",
      "\n",
      "EPOCH 69\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 70\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 71\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 72\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 73\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 74\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 75\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 76\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 77\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 78\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 79\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 80\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 81\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 82\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 83\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 84\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 85\n",
      " Train loss: 0.05035\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 86\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 87\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05067\n",
      "\n",
      "EPOCH 88\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 89\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 90\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 91\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 92\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n",
      "EPOCH 93\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 94\n",
      " Train loss: 0.05033\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 95\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 96\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 97\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05066\n",
      "\n",
      "EPOCH 98\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05064\n",
      "\n",
      "EPOCH 99\n",
      " Train loss: 0.05034\n",
      "   Val loss: 0.05065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network(LSTModel, train_loader, val_loader, criterion, optimizer, nepoch=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyBiLSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x, hc_0):\n",
    "        # hc_0 needs to be a tuple of hidden and cell states, (h_0, c_0)\n",
    "        lstm_outputs, hc_n = self.lstm(x.to(torch.float32), (hc_0[0].to(torch.float32), hc_0[1].to(torch.float32)))\n",
    "        outputs = self.output_layer(lstm_outputs.to(torch.float32))\n",
    "        \n",
    "        return outputs, hc_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 32\n",
    "output_size = 10\n",
    "BiLSTModel = MyBiLSTM(input_size * 2, hidden_size, output_size) # 88 keys on a piano, ten fingers\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(BiLSTModel.parameters(), lr=0.00001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a48052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_network2(model, train_loader, val_loader, criterion, optimizer, nepoch):\n",
    "    try:\n",
    "        for epoch in range(nepoch):\n",
    "            print('EPOCH %d'%epoch)\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            hc_0 = (torch.zeros((2,10,hidden_size)), torch.zeros((2,10,hidden_size)))\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                # print(hc_0[0].detach().dtype)\n",
    "                bi_dir = torch.cat([inputs, torch.flip(inputs, dims=[1])], dim=-1)\n",
    "                # print(bi_dir.shape)\n",
    "                outputs, hc_0 = model(bi_dir, (hc_0[0].detach(), hc_0[1].detach()))\n",
    "                \n",
    "                # print(labels)\n",
    "                labels = F.one_hot(labels, 10).squeeze()\n",
    "                # print('slkjfklds', labels.shape, outputs.shape)\n",
    "                # print(labels)\n",
    "                \n",
    "                # print(outputs[-1])\n",
    "                loss = criterion(outputs[-1].to(torch.float32), labels.to(torch.float32))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                count = 0\n",
    "                for inputs, labels in val_loader:\n",
    "                    bi_dir = torch.cat([inputs, torch.flip(inputs, dims=[1])], dim=-1)\n",
    "                    outputs, hc_0 = model(bi_dir, (hc_0[0].detach(), hc_0[1].detach()))\n",
    "                    labels = F.one_hot(labels, 10).squeeze()\n",
    "                    loss = criterion(outputs[-1], labels)\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "                print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
    "            print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting from training early')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff96f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      " Train loss: 0.12022\n",
      "   Val loss: 0.08551\n",
      "\n",
      "EPOCH 1\n",
      " Train loss: 0.06875\n",
      "   Val loss: 0.05777\n",
      "\n",
      "EPOCH 2\n",
      " Train loss: 0.05410\n",
      "   Val loss: 0.05257\n",
      "\n",
      "EPOCH 3\n",
      " Train loss: 0.05179\n",
      "   Val loss: 0.05161\n",
      "\n",
      "EPOCH 4\n",
      " Train loss: 0.05117\n",
      "   Val loss: 0.05128\n",
      "\n",
      "EPOCH 5\n",
      " Train loss: 0.05086\n",
      "   Val loss: 0.05110\n",
      "\n",
      "EPOCH 6\n",
      " Train loss: 0.05073\n",
      "   Val loss: 0.05100\n",
      "\n",
      "EPOCH 7\n",
      " Train loss: 0.05067\n",
      "   Val loss: 0.05094\n",
      "\n",
      "EPOCH 8\n",
      " Train loss: 0.05064\n",
      "   Val loss: 0.05091\n",
      "\n",
      "EPOCH 9\n",
      " Train loss: 0.05061\n",
      "   Val loss: 0.05090\n",
      "\n",
      "EPOCH 10\n",
      " Train loss: 0.05058\n",
      "   Val loss: 0.05088\n",
      "\n",
      "EPOCH 11\n",
      " Train loss: 0.05056\n",
      "   Val loss: 0.05085\n",
      "\n",
      "EPOCH 12\n",
      " Train loss: 0.05055\n",
      "   Val loss: 0.05086\n",
      "\n",
      "EPOCH 13\n",
      " Train loss: 0.05054\n",
      "   Val loss: 0.05083\n",
      "\n",
      "EPOCH 14\n",
      " Train loss: 0.05053\n",
      "   Val loss: 0.05082\n",
      "\n",
      "EPOCH 15\n",
      " Train loss: 0.05052\n",
      "   Val loss: 0.05081\n",
      "\n",
      "EPOCH 16\n",
      " Train loss: 0.05052\n",
      "   Val loss: 0.05084\n",
      "\n",
      "EPOCH 17\n",
      " Train loss: 0.05051\n",
      "   Val loss: 0.05080\n",
      "\n",
      "EPOCH 18\n",
      " Train loss: 0.05051\n",
      "   Val loss: 0.05081\n",
      "\n",
      "EPOCH 19\n",
      " Train loss: 0.05049\n",
      "   Val loss: 0.05081\n",
      "\n",
      "EPOCH 20\n",
      " Train loss: 0.05050\n",
      "   Val loss: 0.05079\n",
      "\n",
      "EPOCH 21\n",
      " Train loss: 0.05049\n",
      "   Val loss: 0.05081\n",
      "\n",
      "EPOCH 22\n",
      " Train loss: 0.05049\n",
      "   Val loss: 0.05080\n",
      "\n",
      "EPOCH 23\n",
      " Train loss: 0.05048\n",
      "   Val loss: 0.05078\n",
      "\n",
      "EPOCH 24\n",
      " Train loss: 0.05048\n",
      "   Val loss: 0.05080\n",
      "\n",
      "EPOCH 25\n",
      " Train loss: 0.05048\n",
      "   Val loss: 0.05079\n",
      "\n",
      "EPOCH 26\n",
      " Train loss: 0.05048\n",
      "   Val loss: 0.05079\n",
      "\n",
      "EPOCH 27\n",
      " Train loss: 0.05047\n",
      "   Val loss: 0.05078\n",
      "\n",
      "EPOCH 28\n",
      " Train loss: 0.05046\n",
      "   Val loss: 0.05079\n",
      "\n",
      "EPOCH 29\n",
      " Train loss: 0.05046\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 30\n",
      " Train loss: 0.05046\n",
      "   Val loss: 0.05078\n",
      "\n",
      "EPOCH 31\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05078\n",
      "\n",
      "EPOCH 32\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 33\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 34\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 35\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05079\n",
      "\n",
      "EPOCH 36\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 37\n",
      " Train loss: 0.05045\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 38\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 39\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 40\n",
      " Train loss: 0.05044\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 41\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 42\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05077\n",
      "\n",
      "EPOCH 43\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 44\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05078\n",
      "\n",
      "EPOCH 45\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 46\n",
      " Train loss: 0.05043\n",
      "   Val loss: 0.05076\n",
      "\n",
      "EPOCH 47\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 48\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 49\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 50\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 51\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 52\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 53\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 54\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 55\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 56\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 57\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 58\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 59\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05075\n",
      "\n",
      "EPOCH 60\n",
      " Train loss: 0.05042\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 61\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 62\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 63\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 64\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 65\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 66\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 67\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 68\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 69\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 70\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 71\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 72\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 73\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 74\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 75\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 76\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 77\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 78\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 79\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05068\n",
      "\n",
      "EPOCH 80\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 81\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05069\n",
      "\n",
      "EPOCH 82\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 83\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 84\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 85\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 86\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05074\n",
      "\n",
      "EPOCH 87\n",
      " Train loss: 0.05041\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 88\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 89\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 90\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 91\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 92\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 93\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 94\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05070\n",
      "\n",
      "EPOCH 95\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 96\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05071\n",
      "\n",
      "EPOCH 97\n",
      " Train loss: 0.05038\n",
      "   Val loss: 0.05073\n",
      "\n",
      "EPOCH 98\n",
      " Train loss: 0.05039\n",
      "   Val loss: 0.05072\n",
      "\n",
      "EPOCH 99\n",
      " Train loss: 0.05040\n",
      "   Val loss: 0.05069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network2(BiLSTModel, train_loader, val_loader, criterion, optimizer, nepoch=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
